# Optimization-for-Data-Science
Homeworks for the course "Optimization for Data Science" by Alexandre Gramfort and Robert Gower.


## Aims

- *TP1 : First order methods on regression models.* The aim of this material is to code proximal gradient descent (ISTA) accelerated gradient descent (FISTA) for linear regression logistic regression models. The proximal operators we will use are the ridge penalization L1 penalization.

- *TP_NEWTON: (quasi-) Newton methods.* The objective of this lab session is to implement: Newton method, DFP, BFGS and compare your implementation with the BFGS and L-BFGS solvers in scipy.

- *TP_CD : Proximal coordinate descent method on regression models.* The aim of this material is to code proximal coordinate descent for Lasso / L1 linear regression non-negative least squares (NNLS) models. The proximal operators we will use are the L1 penalization
indicator function of $\mathbb{R}_+$

- *Lab3: Logistic and linear regression with deterministic and stochastic first order methods.* The aim of this lab is to implement and compare various batch and stochastic algorithms for linear and logistic with ridge penalization.


## Authors
LÃ©a Bresson (lea.bresson@polytechnique.edu), Eya Kalboussi (eya.kalboussi@polytechnique.edu)
